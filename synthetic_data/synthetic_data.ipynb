{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Generating Synthetic Data with Quantum Randomness: A Comprehensive Guide**\n",
    "\n",
    "*Unlock the power of Quantum Random Number Generators (QRNGs) to enhance your AI models and generate high-quality synthetic data.*\n",
    "\n",
    "---\n",
    "\n",
    "## **Introduction**\n",
    "\n",
    "As artificial intelligence (AI) continues to evolve, the demand for high-quality data grows exponentially. Synthetic data generation has emerged as a viable solution to augment datasets, especially when real data is scarce or sensitive. Traditional methods rely on pseudo-random number generators (PRNGs), which, while efficient, are deterministic and potentially predictable.\n",
    "\n",
    "**Quantum Random Number Generators (QRNGs)** Leveraging the inherent unpredictability of quantum mechanics, QRNGs provide true randomness, offering potential benefits in enhancing model robustness and security.\n",
    "\n",
    "In this tutorial, we'll explore how to integrate QRNGs into a Variational Autoencoder (VAE) for synthetic data generation. We'll cover:\n",
    "\n",
    "- Setting up a QRNG with a fallback mechanism.\n",
    "- Building a QRNG-enhanced VAE.\n",
    "- Training the model on real data.\n",
    "- Generating and evaluating synthetic data.\n",
    "\n",
    "By the end, you'll have a working codebase capable of generating high-quality synthetic data infused with quantum randomness.\n",
    "\n",
    "---\n",
    "\n",
    "## **Why Quantum Randomness in AI?**\n",
    "\n",
    "### **True Randomness vs. Pseudo-Randomness**\n",
    "\n",
    "- **Pseudo-Random Number Generators (PRNGs)**: Generate sequences that appear random but are deterministic if the initial seed is known.\n",
    "- **Quantum Random Number Generators (QRNGs)**: Utilize quantum phenomena to produce numbers that are fundamentally unpredictable.\n",
    "\n",
    "### **Benefits of Integrating QRNGs**\n",
    "\n",
    "- **Enhanced Security**: True randomness reduces vulnerability to attacks exploiting predictable patterns.\n",
    "- **Improved Robustness**: Introducing genuine randomness can prevent overfitting and improve generalization.\n",
    "- **Innovation**: Opens new research avenues at the intersection of quantum computing and AI.\n",
    "\n",
    "---\n",
    "\n",
    "## **Prerequisites**\n",
    "\n",
    "- **Python 3.6+**\n",
    "- **PyTorch**\n",
    "- **NumPy**\n",
    "- **Pandas**\n",
    "- **Scikit-learn**\n",
    "- **Requests library**\n",
    "- **An API token for your Quantum eMotion's Entropy-as-a-Service (EaaS) API**\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Setting Up the Quantum Random Number Generator**\n",
    "\n",
    "First, we'll create a `QuantumRandomGenerator` class to interact with the QRNG API. This class includes a caching mechanism to minimize API calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import requests\n",
    "import base64\n",
    "from datetime import datetime, timedelta\n",
    "from functools import lru_cache\n",
    "\n",
    "class QuantumRandomGenerator:\n",
    "    def __init__(self, api_token: str, cache_ttl_minutes: int = 30):\n",
    "        self.api_token = api_token\n",
    "        self.base_url = 'https://api-qxeaas.quantumemotion.com/entropy' # replace with your QRNG API endpoint\n",
    "        self.headers = {'Authorization': f'Bearer {self.api_token}'}\n",
    "        self.cache_ttl = timedelta(minutes=cache_ttl_minutes)\n",
    "        self._initialize_cache()\n",
    "\n",
    "    def _initialize_cache(self):\n",
    "        @lru_cache(maxsize=32)\n",
    "        def cached_quantum_fetch(num_bytes: int, timestamp: str) -> np.ndarray:\n",
    "            response = requests.get(\n",
    "                self.base_url,\n",
    "                headers=self.headers,\n",
    "                params={'size': num_bytes},\n",
    "                timeout=10\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            qrng_base64 = data['random_number']\n",
    "            qrng_bytes = base64.b64decode(qrng_base64)\n",
    "            return np.frombuffer(qrng_bytes, dtype=np.uint8)\n",
    "        self._cached_fetch = cached_quantum_fetch\n",
    "\n",
    "    def _get_cache_key_timestamp(self) -> str:\n",
    "        now = datetime.now()\n",
    "        ttl_seconds = self.cache_ttl.total_seconds()\n",
    "        epoch_seconds = now.timestamp()\n",
    "        current_period = int(epoch_seconds // ttl_seconds)\n",
    "        period_start = datetime.fromtimestamp(current_period * ttl_seconds)\n",
    "        return period_start.isoformat()\n",
    "\n",
    "    def get_quantum_random(self, num_bytes: int) -> np.ndarray:\n",
    "        if num_bytes > 512:\n",
    "            raise ValueError(\"num_bytes cannot exceed 512\")\n",
    "        cache_timestamp = self._get_cache_key_timestamp()\n",
    "        result = self._cached_fetch(num_bytes, cache_timestamp)\n",
    "        if result is not None:\n",
    "            return result\n",
    "        else:\n",
    "            raise Exception(\"Failed to fetch quantum random numbers\")\n",
    "\n",
    "    def clear_cache(self):\n",
    "        self._cached_fetch.cache_clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Important Notes:**\n",
    "\n",
    "- **API Endpoint**: Replace `'https://api-qxeaas.quantumemotion.com/entropy'` with your actual QRNG API endpoint.\n",
    "- **API Token**: Ensure you have a valid API token and set it appropriately.\n",
    "- **Caching**: Uses an LRU cache to minimize API calls and handle rate limits.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Managing Quantum Random Numbers Efficiently**\n",
    "\n",
    "To handle large requests and minimize API calls, we'll implement a buffer system with the `QuantumRandomBuffer` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantumRandomBuffer:\n",
    "    VALID_SIZES = [4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048, 4096]\n",
    "\n",
    "    def __init__(self, qrng: QuantumRandomGenerator, buffer_size: int = 10000):\n",
    "        self.qrng = qrng\n",
    "        self.buffer_size = buffer_size\n",
    "        self.buffer = np.array([], dtype=np.uint8)\n",
    "\n",
    "    def _get_optimal_chunk_size(self, required_size: int) -> int:\n",
    "        for size in self.VALID_SIZES:\n",
    "            if size >= required_size:\n",
    "                return size\n",
    "        return self.VALID_SIZES[-1]\n",
    "\n",
    "    def get_numbers(self, size: int) -> np.ndarray:\n",
    "        while len(self.buffer) < size:\n",
    "            remaining = size - len(self.buffer)\n",
    "            chunk_size = self._get_optimal_chunk_size(min(512, remaining))\n",
    "            new_numbers = self.qrng.get_quantum_random(chunk_size)\n",
    "            self.buffer = np.concatenate([self.buffer, new_numbers])\n",
    "        result = self.buffer[:size]\n",
    "        self.buffer = self.buffer[size:]\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Features:**\n",
    "\n",
    "- **Buffering**: Stores quantum random numbers to reduce API requests.\n",
    "- **Optimal Chunk Size**: Ensures requests are made in sizes supported by the API.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Building the QRNG-Enhanced Variational Autoencoder**\n",
    "\n",
    "### **3.1 Importing all libraries & Preparing the Dataset**\n",
    "\n",
    "We'll create a custom `TabularDataset` class to handle our tabular data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class TabularDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = torch.FloatTensor(data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **3.2 Implementing the QRNG Variational Autoencoder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QRNGVariationalAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim, hidden_dim, qrng):\n",
    "        super().__init__()\n",
    "        self.qrng = qrng\n",
    "        self.quantum_buffer = QuantumRandomBuffer(qrng)\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.fc_mu = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc_var = nn.Linear(hidden_dim, latent_dim)\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, input_dim)\n",
    "        )\n",
    "\n",
    "    def get_quantum_noise(self, shape):\n",
    "        num_elements = int(np.prod(shape))\n",
    "        qrng_bytes = self.quantum_buffer.get_numbers(num_elements)\n",
    "        noise = (qrng_bytes.astype(np.float32) / 128.0) - 1.0\n",
    "        return torch.FloatTensor(noise.reshape(shape))\n",
    "\n",
    "    def encode(self, x):\n",
    "        hidden = self.encoder(x)\n",
    "        mu = self.fc_mu(hidden)\n",
    "        log_var = self.fc_var(hidden)\n",
    "        return mu, log_var\n",
    "\n",
    "    def reparameterize(self, mu, log_var):\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        eps = self.get_quantum_noise(mu.shape)\n",
    "        if mu.is_cuda:\n",
    "            eps = eps.cuda()\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, log_var = self.encode(x)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        return self.decode(z), mu, log_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Components:**\n",
    "\n",
    "- **Quantum Noise Injection**: Uses quantum random numbers in the reparameterization trick.\n",
    "- **Encoder and Decoder**: Standard VAE architecture with fully connected layers.\n",
    "- **Reparameterization Trick**: Allows backpropagation through stochastic variables.\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Training the Model on Real Data**\n",
    "\n",
    "### **4.1 Preparing the Real Data**\n",
    "\n",
    "We'll use the Breast Cancer Wisconsin dataset from scikit-learn as our real dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "import os\n",
    "\n",
    "# Save breast cancer dataset as CSV\n",
    "cancer = load_breast_cancer()\n",
    "cancer_df = pd.DataFrame(cancer.data, columns=cancer.feature_names)\n",
    "cancer_df.to_csv('breast_cancer.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.2 Initializing the Synthetic Data Generator Class**\n",
    "\n",
    "**Training Details:**\n",
    "\n",
    "- **Loss Function**: Combines reconstruction loss (MSE) and KL divergence.\n",
    "- **Optimizer**: Uses Adam optimizer for efficient training.\n",
    "- **Quantum Noise**: Injected during the reparameterization step.\n",
    "\n",
    "\n",
    "**Generating Synthetic Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SyntheticDataGenerator:\n",
    "    def __init__(self, qrng_token, real_data_path=None):\n",
    "        self.qrng = QuantumRandomGenerator(api_token=qrng_token)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.real_data_path = real_data_path\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    def prepare_data(self, data=None):\n",
    "        if data is None:\n",
    "            if self.real_data_path is None:\n",
    "                raise ValueError(\"Either provide data or set real_data_path\")\n",
    "            data = pd.read_csv(self.real_data_path)\n",
    "\n",
    "        self.original_columns = data.columns\n",
    "        self.original_dtypes = data.dtypes\n",
    "\n",
    "        # Scale the data\n",
    "        scaled_data = self.scaler.fit_transform(data)\n",
    "        return scaled_data\n",
    "\n",
    "    def train_model(self, data, latent_dim=10, hidden_dim=128, batch_size=64, epochs=100):\n",
    "        input_dim = data.shape[1]\n",
    "\n",
    "        # Create dataset and dataloader\n",
    "        dataset = TabularDataset(data)\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        # Initialize model\n",
    "        self.model = QRNGVariationalAutoencoder(\n",
    "            input_dim=input_dim,\n",
    "            latent_dim=latent_dim,\n",
    "            hidden_dim=hidden_dim,\n",
    "            qrng=self.qrng\n",
    "        ).to(self.device)\n",
    "\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=0.001)\n",
    "\n",
    "        # Training loop\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            for batch in dataloader:\n",
    "                batch = batch.to(self.device)\n",
    "\n",
    "                # Forward pass\n",
    "                recon_batch, mu, log_var = self.model(batch)\n",
    "\n",
    "                # Compute loss\n",
    "                recon_loss = nn.MSELoss()(recon_batch, batch)\n",
    "                kl_loss = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "                loss = recon_loss + 0.1 * kl_loss\n",
    "\n",
    "                # Backward pass\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                print(f'Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(dataloader):.4f}')\n",
    "\n",
    "    def generate_samples(self, n_samples):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            # Generate quantum random latent vectors\n",
    "            latent_dim = self.model.fc_mu.out_features\n",
    "            z = self.model.get_quantum_noise((n_samples, latent_dim)).to(self.device)\n",
    "\n",
    "            # Generate samples\n",
    "            generated = self.model.decode(z)\n",
    "            generated = generated.cpu().numpy()\n",
    "\n",
    "            # Inverse transform the generated data\n",
    "            generated_data = self.scaler.inverse_transform(generated)\n",
    "\n",
    "            # Convert to DataFrame with original column names and dtypes\n",
    "            df_generated = pd.DataFrame(generated_data, columns=self.original_columns)\n",
    "            for col, dtype in self.original_dtypes.items():\n",
    "                if np.issubdtype(dtype, np.integer):\n",
    "                    df_generated[col] = df_generated[col].round().astype(dtype)\n",
    "\n",
    "            return df_generated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Steps:**\n",
    "\n",
    "- **Quantum Latent Vectors**: Uses quantum noise to generate latent vectors.\n",
    "- **Decoding**: Transforms latent vectors back to data space.\n",
    "- **Post-processing**: Inverse scaling and type casting to match original data.\n",
    "\n",
    "---\n",
    "\n",
    "## **6. Evaluating the Synthetic Data**\n",
    "\n",
    "We'll compare the statistical properties of the real and synthetic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_synthetic_data_generation(real_data_path, qrng_token, n_samples=1000):\n",
    "    \"\"\"\n",
    "    Demonstrates the synthetic data generation process using a real dataset\n",
    "    \"\"\"\n",
    "    # Initialize generator\n",
    "    generator = SyntheticDataGenerator(qrng_token=qrng_token, real_data_path=real_data_path)\n",
    "\n",
    "    # Load and prepare data\n",
    "    real_data = pd.read_csv(real_data_path)\n",
    "    prepared_data = generator.prepare_data(real_data)\n",
    "\n",
    "    # Train the model\n",
    "    generator.train_model(\n",
    "        data=prepared_data,\n",
    "        latent_dim=min(10, prepared_data.shape[1]),\n",
    "        hidden_dim=128,\n",
    "        epochs=500\n",
    "    )\n",
    "\n",
    "    # Generate synthetic samples\n",
    "    synthetic_data = generator.generate_samples(n_samples)\n",
    "\n",
    "    # Print summary statistics comparison\n",
    "    print(\"\\nReal vs Synthetic Data Summary:\")\n",
    "    print(\"\\nReal Data Summary:\")\n",
    "    print(real_data.describe())\n",
    "    print(\"\\nSynthetic Data Summary:\")\n",
    "    print(synthetic_data.describe())\n",
    "\n",
    "    synthetic_data.to_csv('synthetic_data2.csv')\n",
    "\n",
    "    return synthetic_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Usage Example:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data\n",
    "synthetic_data = demonstrate_synthetic_data_generation(\n",
    "    real_data_path='breast_cancer.csv',\n",
    "    qrng_token=os.getenv('API_TOKEN'),\n",
    "    n_samples=500\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluation Metrics:**\n",
    "\n",
    "- **Statistical Similarity**: Compare mean, standard deviation, and other statistics.\n",
    "- **Data Distribution**: Ensure the synthetic data follows similar distributions as the real data.\n",
    "\n",
    "---\n",
    "\n",
    "## **7. Conclusion**\n",
    "\n",
    "In this tutorial, we've successfully integrated quantum randomness into a Variational Autoencoder for synthetic data generation. By leveraging QRNGs, we've introduced true randomness into the model, potentially enhancing the quality and security of the generated data.\n",
    "\n",
    "**Key Takeaways:**\n",
    "\n",
    "- **QRNG Integration**: Provides true randomness, enhancing unpredictability.\n",
    "- **VAE Architecture**: Effective for generating high-dimensional synthetic data.\n",
    "- **Potential Applications**: Data augmentation, privacy-preserving data sharing, and more.\n",
    "\n",
    "**Next Steps:**\n",
    "\n",
    "- **Experimentation**: Try different datasets and observe the effects.\n",
    "- **Hyperparameter Tuning**: Adjust latent dimensions, hidden sizes, and training epochs.\n",
    "- **Further Research**: Explore integrating QRNGs into other models like GANs.\n",
    "\n",
    "---\n",
    "\n",
    "## **References**\n",
    "\n",
    "- **Quantum Random Number Generators**: [Wikipedia](https://en.wikipedia.org/wiki/Quantum_random_number_generator)\n",
    "- **Auto-Encoding Variational Bayes**: [Kingma & Welling (2013)](https://arxiv.org/abs/1312.6114)\n",
    "- **PyTorch Documentation**: [PyTorch Official Site](https://pytorch.org/docs/stable/index.html)\n",
    "- **Breast Cancer Wisconsin Dataset**: [UCI Machine Learning Repository](https://archive.ics.uci.edu/dataset/17/breast+cancer+wisconsin+diagnostic)\n",
    "\n",
    "---\n",
    "\n",
    "*Embrace the future of AI by integrating quantum technologies into your models today! If you found this tutorial helpful, please share it with others interested in the exciting intersection of quantum computing and artificial intelligence.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
