{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Utilizing Quantum Randomness as Noise in Neural Networks: A Practical Guide**\n",
    "\n",
    "*Unlock the power of Quantum Random Number Generators (QRNG) to enhance your AI models.*\n",
    "\n",
    "![Quantum AI](https://quantumai.google/static/site-assets/images/marketing/systems/hero.jpg)\n",
    "\n",
    "---\n",
    "\n",
    "## **Introduction**\n",
    "\n",
    "As artificial intelligence (AI) continues to evolve, the quest for integrating cutting-edge technologies into AI models intensifies. One such frontier is the incorporation of **Quantum Random Number Generators (QRNGs)** into neural networks. Unlike classical pseudo-random number generators (PRNGs), QRNGs harness the inherent unpredictability of quantum mechanics to produce true randomness.\n",
    "\n",
    "In this tutorial, we'll explore how to integrate QRNGs into a neural network designed to recognize handwritten digits using the MNIST dataset. We'll walk through the essential components of the codebase, explain how quantum randomness is infused into the model, and demonstrate the potential benefits of this approach.\n",
    "\n",
    "---\n",
    "\n",
    "## **Why Quantum Randomness in AI?**\n",
    "\n",
    "Traditional PRNGs are algorithmically determined and can, in theory, be predicted if the initial seed is known. QRNGs, on the other hand, rely on quantum phenomena, making their outputs fundamentally unpredictable. Integrating QRNGs into AI models can:\n",
    "\n",
    "- **Enhance Security**: Improve resistance against attacks that exploit predictable randomness.\n",
    "- **Increase Robustness**: Introduce true randomness to prevent overfitting and improve generalization.\n",
    "- **Innovation**: Open new avenues for research at the intersection of quantum computing and AI.\n",
    "\n",
    "---\n",
    "\n",
    "## **Overview of the Implementation**\n",
    "\n",
    "We'll build a simple neural network with quantum randomness integrated into:\n",
    "\n",
    "1. **QuantumRandomGenerator**: Fetches quantum random numbers from a QRNG API.\n",
    "2. **QuantumRandomBuffer**: Manages quantum random numbers efficiently.\n",
    "3. **QRNGLayer**: A neural network layer that injects quantum noise into its computations.\n",
    "4. **QuantumNeuralNetwork**: The full model utilizing QRNG layers.\n",
    "\n",
    "---\n",
    "\n",
    "## **Prerequisites**\n",
    "\n",
    "- Python 3.6 or higher\n",
    "- PyTorch\n",
    "- NumPy\n",
    "- Requests library\n",
    "- Scikit-learn\n",
    "- Seaborn\n",
    "- Matplotlib\n",
    "- An API token for your Quantum eMotion's Entropy-as-a-Service (EaaS) API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import requests\n",
    "import base64\n",
    "from datetime import datetime, timedelta\n",
    "from functools import lru_cache\n",
    "from typing import Tuple\n",
    "from torchvision import datasets, transforms\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Setting Up the Quantum Random Generator**\n",
    "\n",
    "First, we create a class to interact with the QRNG API and fetch random numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class QuantumRandomGenerator:\n",
    "    def __init__(self, api_token: str, cache_ttl_minutes: int = 30):\n",
    "        self.api_token = api_token\n",
    "        self.base_url = 'https://api-qxeaas.quantumemotion.com/entropy'\n",
    "        self.headers = {'Authorization': f'Bearer {self.api_token}'}\n",
    "        self.cache_ttl = timedelta(minutes=cache_ttl_minutes)\n",
    "        self._initialize_cache()\n",
    "        \n",
    "    def _initialize_cache(self):\n",
    "        @lru_cache(maxsize=32)\n",
    "        def cached_quantum_fetch(num_bytes: int, timestamp: str) -> np.ndarray:\n",
    "            response = requests.get(\n",
    "                self.base_url,\n",
    "                headers=self.headers,\n",
    "                params={'size': num_bytes},\n",
    "                timeout=10\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            qrng_base64 = data['random_number']\n",
    "            qrng_bytes = base64.b64decode(qrng_base64)\n",
    "            return np.frombuffer(qrng_bytes, dtype=np.uint8)\n",
    "        self._cached_fetch = cached_quantum_fetch\n",
    "        \n",
    "    def _get_cache_key_timestamp(self) -> str:\n",
    "        now = datetime.now()\n",
    "        ttl_seconds = self.cache_ttl.total_seconds()\n",
    "        epoch_seconds = now.timestamp()\n",
    "        current_period = int(epoch_seconds // ttl_seconds)\n",
    "        period_start = datetime.fromtimestamp(current_period * ttl_seconds)\n",
    "        return period_start.isoformat()\n",
    "        \n",
    "    def get_quantum_random(self, num_bytes: int) -> np.ndarray:\n",
    "        if num_bytes > 512:\n",
    "            raise ValueError(\"num_bytes cannot exceed 512\")\n",
    "        cache_timestamp = self._get_cache_key_timestamp()\n",
    "        result = self._cached_fetch(num_bytes, cache_timestamp)\n",
    "        if result is not None:\n",
    "            return result\n",
    "        else:\n",
    "            raise Exception(\"Failed to fetch quantum random numbers\")\n",
    "        \n",
    "    def clear_cache(self):\n",
    "        self._cached_fetch.cache_clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Points:**\n",
    "\n",
    "- **API Interaction**: The class communicates with the QRNG service using HTTP requests.\n",
    "- **Data Handling**: Fetches random bytes and converts them into a NumPy array.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Efficient Quantum Random Number Management**\n",
    "\n",
    "To handle large requests efficiently, we implement a buffer system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class QuantumRandomBuffer:\n",
    "    VALID_SIZES = [4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048, 4096]\n",
    "    \n",
    "    def __init__(self, qrng: QuantumRandomGenerator, buffer_size: int = 10000):\n",
    "        self.qrng = qrng\n",
    "        self.buffer_size = buffer_size\n",
    "        self.buffer = np.array([], dtype=np.uint8)\n",
    "        \n",
    "    def _get_optimal_chunk_size(self, required_size: int) -> int:\n",
    "        for size in self.VALID_SIZES:\n",
    "            if size >= required_size:\n",
    "                return size\n",
    "        return self.VALID_SIZES[-1]\n",
    "        \n",
    "    def get_numbers(self, size: int) -> np.ndarray:\n",
    "        while len(self.buffer) < size:\n",
    "            remaining = size - len(self.buffer)\n",
    "            chunk_size = self._get_optimal_chunk_size(min(512, remaining))\n",
    "            new_numbers = self.qrng.get_quantum_random(chunk_size)\n",
    "            self.buffer = np.concatenate([self.buffer, new_numbers])\n",
    "        result = self.buffer[:size]\n",
    "        self.buffer = self.buffer[size:]\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Points:**\n",
    "\n",
    "- **Buffering**: Reduces the number of API calls by fetching larger chunks of random numbers.\n",
    "- **Optimal Chunk Size**: Adjusts the request size based on API limitations.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Building Quantum-Enhanced Neural Network Layers**\n",
    "\n",
    "We create a custom layer that injects quantum randomness into the inputs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class QRNGLayer(nn.Module):\n",
    "    def __init__(self, input_size: int, output_size: int, qrng: QuantumRandomGenerator):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_size, output_size)\n",
    "        self.quantum_buffer = QuantumRandomBuffer(qrng)\n",
    "        self.input_size = input_size\n",
    "        \n",
    "    def get_quantum_noise(self, shape: Tuple[int, ...]) -> torch.Tensor:\n",
    "        num_elements = int(np.prod(shape))\n",
    "        qrng_bytes = self.quantum_buffer.get_numbers(num_elements)\n",
    "        noise = (qrng_bytes.astype(np.float32) / 128.0) - 1.0\n",
    "        return torch.FloatTensor(noise.reshape(shape))\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, noise_scale: float = 0.1) -> torch.Tensor:\n",
    "        batch_size = x.shape[0]\n",
    "        noise = self.get_quantum_noise((batch_size, self.input_size))\n",
    "        if x.is_cuda:\n",
    "            noise = noise.cuda()\n",
    "        noisy_input = x + noise_scale * noise\n",
    "        return self.linear(noisy_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Points:**\n",
    "\n",
    "- **Noise Injection**: Adds quantum noise to the input features.\n",
    "- **Scalability**: Supports GPU acceleration by moving tensors to CUDA if available.\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Constructing the Quantum Neural Network**\n",
    "\n",
    "We assemble the network using our custom QRNG layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class QuantumNeuralNetwork(nn.Module):\n",
    "    def __init__(self, qrng: QuantumRandomGenerator):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.qrng_layer1 = QRNGLayer(784, 128, qrng)\n",
    "        self.qrng_layer2 = QRNGLayer(128, 64, qrng)\n",
    "        self.qrng_layer3 = QRNGLayer(64, 10, qrng)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, noise_scale: float = 0.1) -> torch.Tensor:\n",
    "        x = self.flatten(x)\n",
    "        x = self.dropout(self.relu(self.qrng_layer1(x, noise_scale)))\n",
    "        x = self.dropout(self.relu(self.qrng_layer2(x, noise_scale)))\n",
    "        x = self.qrng_layer3(x, noise_scale)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Points:**\n",
    "\n",
    "- **Architecture**: Designed for the MNIST dataset with input size 28x28 pixels.\n",
    "- **Activation and Regularization**: Uses ReLU activations and dropout for better generalization.\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Training the Model**\n",
    "\n",
    "### **Preparing the Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST('data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST('data', train=False, transform=transform)\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Setting Up Training Components**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "qrng = QuantumRandomGenerator(api_token=os.getenv(\"API_TOKEN\"))  # Replace with your API token\n",
    "model = QuantumNeuralNetwork(qrng).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## **6. Evaluating the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    test_loss = 0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            all_preds.extend(pred.cpu().numpy())\n",
    "            all_labels.extend(target.cpu().numpy())\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "    return accuracy, conf_matrix, test_loss / len(test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Points:**\n",
    "\n",
    "- **Inference Mode**: Disables gradient computation for efficiency.\n",
    "- **Accuracy Measurement**: Uses scikit-learn for calculating accuracy.\n",
    "\n",
    "---\n",
    "\n",
    "## **7. Visualizing the Results**\n",
    "\n",
    "You can plot a confusion matrix to visualize the model's performance across different classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(conf_matrix, save_path='confusion_matrix.png'):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **Training and Testing Loop**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "epochs = 10\n",
    "best_accuracy = 0\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    accuracy, conf_matrix, test_loss = evaluate_model(model, test_loader, device)\n",
    "    scheduler.step(test_loss)\n",
    "    \n",
    "    # Print logs\n",
    "    print(f'Epoch {epoch + 1}/{epochs}')\n",
    "    print(f'    Training Loss: {total_loss / len(train_loader):.4f}')\n",
    "    print(f'    Test Loss: {test_loss:.4f}')\n",
    "    print(f'    Test Accuracy: {accuracy:.4f}')\n",
    "    \n",
    "    # Save best model and plot confusion matrix\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        torch.save(model.state_dict(), 'best_quantum_model.pth')\n",
    "        plot_confusion_matrix(conf_matrix)\n",
    "        print('    Best model saved.')\n",
    "\n",
    "print(f'Training completed. Best accuracy: {best_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Points:**\n",
    "\n",
    "- **Device Configuration**: Utilizes GPU if available for faster training.\n",
    "- **Loss Calculation**: Uses cross-entropy loss suitable for multi-class classification.\n",
    "\n",
    "---\n",
    "\n",
    "## **Conclusion**\n",
    "\n",
    "In this tutorial, we've demonstrated how to integrate quantum randomness into a neural network. By injecting quantum noise into the inputs of our custom layers, we aim to enhance the model's robustness and generalization capabilities.\n",
    "\n",
    "**Potential Benefits:**\n",
    "\n",
    "- **Improved Generalization**: The stochastic nature may help prevent overfitting.\n",
    "- **Enhanced Security**: True randomness can make models more resistant to certain types of attacks.\n",
    "\n",
    "**Next Steps:**\n",
    "\n",
    "- **Experiment with Different Architectures**: Try deeper networks or convolutional layers.\n",
    "- **Adjust Noise Scaling**: Fine-tune the `noise_scale` parameter to observe its effect.\n",
    "- **Apply to Other Datasets**: Test the approach on more complex datasets like CIFAR-10 or ImageNet.\n",
    "\n",
    "---\n",
    "\n",
    "## **References**\n",
    "\n",
    "- **Quantum Random Number Generators**: [Wikipedia](https://en.wikipedia.org/wiki/Quantum_random_number_generator)\n",
    "- **PyTorch Documentation**: [PyTorch Official Site](https://pytorch.org/docs/stable/index.html)\n",
    "- **MNIST Dataset**: [Yann LeCun's Website](https://yann.lecun.com/exdb/mnist/)\n",
    "\n",
    "---\n",
    "\n",
    "*Embrace the future by integrating quantum technologies into your AI models today!*\n",
    "\n",
    "---\n",
    "\n",
    "*If you found this tutorial helpful, don't forget to clap and share it with your fellow developers interested in quantum computing and AI.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
